{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 Divide and Conquer\n",
    "\n",
    "In this module we will continue the discussion of some low-level concepts for high-performance computing of data.\n",
    "We will example some lower-level programming concepts related to parallel computation,\n",
    "especially as relevant to modern data analytics such as deep learning and AI.\n",
    "\n",
    "In subsequent modules, we will move to larger distributed systems for our analytics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Videos\n",
    "\n",
    "* [Divide and Conquer (11 min)](https://youtu.be/kcQi6npy-qI)\n",
    " * [Slides](./resources/DSA8430_Divide_and_Conquer.pdf)\n",
    "* [What is CUDA and GPU Computing (5 min)](https://youtu.be/IzU4AVcMFys)\n",
    "* These two videos are from the Computer Science/Computer Engineering HPC course (see additional related videos below)\n",
    "  * [Parallel Patterns Reduction (10 min)](https://umsystem.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=80929a04-6a79-4fe8-a9e4-acd8000121f4)\n",
    "  * [Parallel Patterns Stencil (17 min)](https://umsystem.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=2cf99cc1-c073-4d2e-a642-acd8000121f4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Readings\n",
    "\n",
    "  * [How GPUs Accelerate Deep Learning](https://towardsdatascience.com/how-gpus-accelerate-deep-learning-3d9dec44667a)\n",
    "\n",
    "\n",
    "\n",
    "### Extra/Optional Videos from Dr. Scott's HPC Computer Science/Computer Engineering course\n",
    "\n",
    "Note: These videos are from the second half of an advanced Computer Science/Computer Engineering Course in low-level parallel programming for High-Performance Computing.\n",
    "\n",
    " * [Parallel Patterns Introduction (3 min)](https://umsystem.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=d9e54503-8463-4b2e-aad1-adcf0142677d)\n",
    " * [Parallel Patterns Fork and Join (4 min)](https://umsystem.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=9dd97ab1-5068-4988-a240-adcf01428ffc)\n",
    " * [Parallel Patterns Geometric Decomposition (6 min)](https://umsystem.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=514388df-bd41-4737-91ac-acd8000121f4)\n",
    " * [Parallel Patterns Pipelines (5 min)](https://umsystem.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=bbd2fe12-b38c-48fe-bac0-acd8000121f4)\n",
    " \n",
    "Note: These videos are from the last month of an advanced Computer Science/Computer Engineering Course in low-level parallel programming for High-Performance Computing.\n",
    " \n",
    " * [General Purpose GPU Programming (38 min)](https://umsystem.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=c2afdf0d-56ea-47b9-bd47-add300f7ad7d)\n",
    " * [GPU Grid/Block/Threads (49 min)](https://umsystem.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=6c8f2d7e-9727-4c3f-a80e-add4003cc2af)\n",
    "\n",
    "\n",
    "#### Other Extra Videos on GPU, TensorFlow, etc.\n",
    "* [Tensor Cores to Accelerate Machine Learning (4 min)](https://youtu.be/yyR0ZoCeBO8)\n",
    "* [TensorFlow Channel](https://www.youtube.com/channel/UC0rqucBdTuFTjJiefW5t-IQ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labs\n",
    "\n",
    "* [CNN on DSA Europa: Tensorflow - CPU](./labs/MNIST_DSA.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practices\n",
    "\n",
    "* [CNN on NSF Nautilus: Stack Tensorflow](./practices/MNIST_Nautilus.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercises \n",
    "### A. [Convolutional Neural Network on Nautilus](./exercses/cnn.ipynb)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Choose one of the links below and complete the lab. \n",
    "\n",
    "#### Extra \"Labs\" in the Tensorflow Colab environment\n",
    " * Decision Forests: https://www.tensorflow.org/decision_forests/tutorials/beginner_colab\n",
    " * Transfer Learning: https://www.tensorflow.org/tutorials/images/transfer_learning\n",
    " * Time Series: https://www.tensorflow.org/tutorials/structured_data/time_series\n",
    " * Text Classification: https://www.tensorflow.org/text/tutorials/text_classification_rnn\n",
    " \n",
    "\n",
    "Once you have completed the lab, write a brief summary of what elements of Tensorflow were used and how you estimate/guess-timate that parallelization is being leveraged within the examples."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#  Add your Exercise 2.B summary below this line.\n",
    "\n",
    "For my choice in B I took a look at the Decision Forest link which built models from the TensorFlow Decision Tree library. Since we have done a lot of classification work and some decision tree work in other classes this one initially seemed a little closer to home to understand. One piece that was interesting to me was how the TensorFlow version did not have to specifiy any sort of additional data carpentry around categorical values (e.g. one-hot encoding). From there it followed similar paths to what we have done in the past with classification models, train-test-split (though this appeared to be more manual than just the simple package in sklearn), adjust hyperparameters where needed, train and evaluate the models. Honestly after running the lab this one didn't feel that different compared to other RFC's we have run, which may also be due to the fact that the datasets were so small. \n",
    "\n",
    "\n",
    "My guess for the parallelization aspect of the question is a fork-join technique. I mainly say that as fork-join seems closely related to how decision forest/trees work in that you split the problem into smaller trees and the take the mean or some other combination of the outputs from each tree and join back to the results. Decision forests seems like a really good place for optimization from parallelization as running each tree simultanesoulsy could significantly cut back on runtime when we make larger forests. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting your work\n",
    "\n",
    "### <span style='background:blue'>Please be sure the artifacts from all practices and exercises are added into your repository for the commit and push!</span>\n",
    "\n",
    "#### Steps:\n",
    "  1. Open Terminal in JupyterHub\n",
    "  1. Change into the course folder\n",
    "  1. Stage (Git Add) the module's learning activities   \n",
    "  `git  add   module2`\n",
    "  1. Create your work snapshot (Git Commit)  \n",
    "  `git   commit   -m   \"Module 2 submission\"`\n",
    "  1. Upload the snapshot to the server (Git Push)  \n",
    "  `git   push`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

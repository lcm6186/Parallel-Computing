{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 6 - Advanced Distributed Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module investigates the linkage of prior technologies and concepts for supporting advanced parallel data processing Map-Reduce based computation (Spark).\n",
    "### Topics\n",
    " 1. Map-Reduce\n",
    " 2. Hadoop\n",
    " 3. Spark\n",
    " 4. GCP Dataproc\n",
    " \n",
    "\n",
    "#### Module Kickoff Video\n",
    "* [Introduction to Map-Reduce, Hadoop, Spark, and Dataproc (12 min)](https://youtu.be/RV4TWYefZwo)\n",
    " * [Slides](./resources/DSA8430_Parallel_AdvancedDist.pdf)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Videos\n",
    "\n",
    "To get a little taste of the relationship of Map Reduce, Hadoop, Spark, and GCP Dataproc please watch these first short video.\n",
    " * GCP Dataproc (1 min): https://youtu.be/Jj6mp7Sam10\n",
    " * Introduction to Map Reduce and Hadoop (6 min): https://youtu.be/aReuLtY0YMI\n",
    " * Introduction to Spark (38 min): https://youtu.be/znBa13Earms\n",
    " * GCP Dataproc as Hadoop/Spark (3 min): https://youtu.be/h1LvACJWjKc\n",
    "\n",
    "Suggested Additional Viewings\n",
    " * Map Reduce (35 min): https://youtu.be/b-IvmXoO0bU\n",
    " * GCP Dataproc (47 min): https://youtu.be/IgnwXDU770M\n",
    " * PySpark vs Pandas (31 min): https://youtu.be/b-IvmXoO0bU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup (Lab)\n",
    "\n",
    "Remember to access GCP through our special IAM portal.  See [GCP Getting Started](https://europa.dsa.missouri.edu/user/tpgd5/notebooks/ParallelProgrammingAnalytics/module1/practices/GCP_Getting_Started.ipynb) for details.\n",
    "\n",
    "### [Enable the appropriate APIs for Dataproc, etc.](https://console.cloud.google.com/apis/enableflow?apiid=dataproc,bigquery.googleapis.com,compute_component&_ga=2.131884943.1006469968.1647351992-382265758.1640621151&project=umc-dsa-8430-sp2022)\n",
    "\n",
    "\n",
    "Ensure you see the correct project, `umc-dsa-8430-sp2022`\n",
    "\n",
    "![GCP_Dataproc_Enable_1.png MISSING](./images/GCP_Dataproc_Enable_1.png)\n",
    "\n",
    "You should then see\n",
    "\n",
    "> You are about to enable:\n",
    ">   \n",
    "> Cloud Dataproc API  \n",
    "> BigQuery API  \n",
    "> Compute Engine API\n",
    "\n",
    "Click **ENABLE**\n",
    "\n",
    "\n",
    "## Lab - BigQuery Revisited\n",
    "\n",
    "#### Estimated Tutorial Time: <span style='color:blue'>approximately 15 minutes</span>\n",
    "\n",
    "Follow the tutorial here: https://cloud.google.com/dataproc/docs/tutorials/bigquery-sparkml#create_a_subset_of_bigquery_natality_data\n",
    "\n",
    "We are just working down until you have created your `regression_input` table within your newly created dataset.\n",
    "\n",
    "**Don't forget to use your SSO to prefix resources you create in the project!**\n",
    "\n",
    "**Additionally, set your table to expire in 30 days!**\n",
    "\n",
    "#### Saving the Query Results to Table in your Dataset Should look similar to below:\n",
    "\n",
    "![GCP_BigQuery_Result_to_Datatable.png MISSING](./images/GCP_BigQuery_Result_to_Datatable.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practices and Tutorials\n",
    "\n",
    "The practices for this module involve following some of the select tutorials from the User Guide.\n",
    "We then ask you to do a few extra steps to practice your data science skills.\n",
    "\n",
    "For each practice, ensure your artifacts are properly linked into this notebook and uploaded in the appropriate location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Status Check:  Create a screen snip of your Table Info similar to below\n",
    "\n",
    "![GCP_BigQuery_Datatable_Info.png is MISSING](./images/GCP_BigQuery_Datatable_Info.png)\n",
    "\n",
    "##### Artifact - Your image `regression_input.png` should be linked below\n",
    "\n",
    "![Your regression_input.png image is MISSING](regression_input.png)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Cloud Shell\n",
    "For some of the activities in this module, we will be using the **Google Cloud Shell**.\n",
    "\n",
    "You can launch the Google Cloud Shell from the GCP Console using the icon noted below.\n",
    "![GCP_GoogleCloudShell_launch.png MISSING](./images/GCP_GoogleCloudShell_launch.png)\n",
    "\n",
    "---\n",
    "\n",
    "Once you activate the Google Cloud Shell, you will get a Linux Terminal view a the bottom of that browser tab.\n",
    "This is from a container launced in the GCP, similar to how you accessed pods in the Kubernetes module.\n",
    "\n",
    "![GCP_GoogleCloudShell_initialized.png MISSING](./images/GCP_GoogleCloudShell_initialized.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Practice 1:\n",
    "\n",
    "#### Estimated Time: <span style='color:blue'>approximately 20 minutes</span>\n",
    "\n",
    "Create a Dataproc cluster using the Console\n",
    "\n",
    "https://console.cloud.google.com/dataproc/clustersAdd?_ga=2.130526095.1006469968.1647351992-382265758.1640621151\n",
    "\n",
    "Name your cluster `cluster-SSO`, substituting your actual SSO.\n",
    "\n",
    "Leave the type as _Standard (1 master, N workers)_\n",
    "\n",
    "Check the box to _Enable component Gateway_\n",
    "\n",
    "For _Option components_, sekect **Jupyter Notebook**\n",
    "\n",
    "Leave all the default options under _Configure Nodes_, _Customize Cluster_, and _Manage Security_.\n",
    "\n",
    "Click the **Create** button when you are ready!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice 1 - Artifact\n",
    "\n",
    "#### 1. From the Cloud Shell, export the YAML file that defines/configures your cluster! \n",
    "Example (guess what you need to do with SSO below):\n",
    "```BASH\n",
    "gcloud dataproc  clusters export cluster-SSO --destination cluster-SSO.yaml --region us-central1\n",
    "```\n",
    "\n",
    "#### 2. After the YAML file is produced, download it from GCP using the download option from the Google Cloud Shell.\n",
    "\n",
    "#### 3. Then, upload it into this module's `practices/` folder.\n",
    "\n",
    "\n",
    "#### 4. Update the link below to point to your Cluster YAML file.\n",
    "Test that the link does not produce a 404 error (and therefor zero points).\n",
    "\n",
    "[cluster-SSO.yaml](./practices/cluster-lcmhng.yaml)\n",
    "\n",
    "\n",
    "#### 5. Delete your Cluster!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Practice 2\n",
    "\n",
    "#### Estimated Time: <span style='color:blue'>approximately 60 minutes</span>\n",
    "\n",
    "\n",
    "#### 1. We will now recreate the cluster using the Google Cloud Shell, but add a suffix of `-2` to the cluster name.\n",
    "\n",
    "```BASH\n",
    "gcloud dataproc clusters import cluster-SSO-2 --source cluster-SSO.yaml --region us-central1\n",
    "```\n",
    "\n",
    "This will take a little bit to run, but initially you should see output similar to below:\n",
    "```BASH\n",
    "Waiting on operation [projects/umc-dsa-8430-sp2022/regions/us-central1/operations/36783211-573b-362d-b525-64dff1987991].\n",
    "Waiting for cluster creation operation...\n",
    "WARNING: For PD-Standard without local SSDs, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See https://cloud.google.com/compute/docs/disks/performance for information on disk I/O performance.\n",
    "Waiting for cluster creation operation...working   \n",
    "```\n",
    "\n",
    "Eventually, you should see a message similar to:\n",
    "```BASH\n",
    "Created [https://dataproc.googleapis.com/v1/projects/umc-dsa-8430-sp2022/regions/us-central1/clusters/cluster-scottgs-2] Cluster placed in zone [us-central1-a].\n",
    "```\n",
    "And the cluster will be visible in the Console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice 2 - Artifact - A\n",
    "\n",
    "Click on your running cluster and get a screen snip of the top details (Name, Cluster UUID, Type, and Status ... showing the cluster in Running state).\n",
    "Name your screen snip `practice2a.png`, and save it into the module `practices` folder. \n",
    "Ensure your screen snip shows in the cell below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Your ./practices/practice2a.png is MISSING](./practices/practice2a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create a Jupyter Notebook from BigQuery Spark-ML Linear Regression tutorial code.\n",
    "\n",
    "The code is found at the link below.\n",
    "\n",
    "https://cloud.google.com/dataproc/docs/tutorials/bigquery-sparkml#run_a_linear_regression\n",
    "\n",
    "\n",
    "As you make your Notebook with multiple Code Cells, \n",
    "be sure to add Markdown Cells and copy code comments into Markdown to allow a future user (or your future self) to read back through and understand what is going on.\n",
    "\n",
    "##### Important Change from Tutorial\n",
    "\n",
    "Update the way the Spark Session is instantiated.\n",
    "This changes the way that the Java Archive (JAR) is loaded into memory for your PySpark session.\n",
    "\n",
    "```\n",
    "#sc = SparkContext()\n",
    "#spark = SparkSession(sc)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "  .config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar') \\\n",
    "  .getOrCreate()\n",
    "```\n",
    "\n",
    "**Also make sure you update the SQL** to be appropriate for your dataset and table name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice 2 - Artifact - B\n",
    "\n",
    "Name your notebook `SSO_SparkML.ipynb`, using your SSO instead of literal characters `SSO`.\n",
    "**Ensure the output is saved in the notebook.**\n",
    "\n",
    "Upload the notebook into the module `practices` folder, then update the link in the cell below to point to your uploaded file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your Spark ML Notebook](./practices/lcmhng_SparkML.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Delete your Dataproc cluster (cluster-SSO-2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1:\n",
    "\n",
    "#### Estimated Time: <span style='color:blue'>approximately 60 minutes</span>\n",
    "\n",
    "Spin up a new cluster, **cluster-SSO-3**.\n",
    "\n",
    "Briefly review your work on the [BigQuery Exercise in Module 3](../module3/GCP_BigQuery.ipynb#Exercise)\n",
    "\n",
    "Re-use or re-stage the Big Query dataset.\n",
    "In module 3, you created Logistic Regression model in BigQuery.\n",
    "\n",
    "For this exercise, build a notebook modeled off of the Practice 2, but using a **classification model** of you choice for the `google_analytics_sample` data.\n",
    "https://spark.apache.org/docs/3.1.2/api/python/reference/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name your notebook `SSO_SparkClassification.ipynb`, using your SSO instead of literal characters `SSO`.\n",
    "**Ensure the output is saved in the notebook.**\n",
    "\n",
    "Upload the notebook into the module `exercises` folder, then update the link in the cell below to point to your uploaded file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your Spark Classification Notebook](./exercises/lcmhng_SparkClassification.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2:\n",
    "\n",
    "#### Estimated Time: <span style='color:blue'>approximately 90 minutes</span>\n",
    "\n",
    "Briefly review your work on the [AWS Quicksight Exercise in Module 4](../module4/AWS_Quicksight.ipynb#Excercises)\n",
    "\n",
    "Using the same dataset, if possible, create a new BigQuery dataset.\n",
    "For this exercise, use [Spark ML](https://spark.apache.org/docs/3.1.2/api/python/reference/) to perform one of the following options on your data.\n",
    " * Classification (not the same as you used in Exercise 1)\n",
    " * Regression (not the same as you used in Practice above)\n",
    " * Clustering Analysis\n",
    " \n",
    "Be sure to add some analytics and explanations of your data, models, and findings.\n",
    "Visualizations are good!\n",
    " \n",
    "Name your notebook `SSO_SparkExercise.ipynb`, using your SSO instead of literal characters `SSO`.\n",
    "**Ensure the output is saved in the notebook.**\n",
    "\n",
    "Upload the notebook into the module `exercises` folder, then update the link in the cell below to point to your uploaded file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your Spark Exercises Notebook](./exercises/lcmhng_SparkExercise.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3:\n",
    "\n",
    "Delete you Dataproc cluster and your BigQuery Datasets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Save your Notebook and commit the Notebook and artifacts for Grading\n",
    "\n",
    "---\n",
    "\n",
    "## Submitting your work\n",
    "\n",
    "### <span style='background:lightblue'>Please be sure the artifacts from all practices and exercises are added into your repository for the commit and push!</span>\n",
    "\n",
    "#### Steps:\n",
    "  1. Open Terminal in JupyterHub\n",
    "  1. Change into the course folder\n",
    "  1. Stage (Git Add) the module's learning activities   \n",
    "  `git  add   module6`\n",
    "  1. Create your work snapshot (Git Commit)  \n",
    "  `git   commit   -m   \"Module 6 submission\"`\n",
    "  1. Upload the snapshot to the server (Git Push)  \n",
    "  `git   push`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

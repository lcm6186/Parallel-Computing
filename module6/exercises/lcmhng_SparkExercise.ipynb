{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e95abdb",
   "metadata": {},
   "source": [
    "## Data Set - 311\n",
    "\n",
    "\n",
    "For this exercise I have chosen to run with the 311 data that we created a while back. This dataset and the ability to reference it made more sense to me and there should be enough data in there to run interesting models on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3015217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "# I'm starting with a bunch as I'm not sure \n",
    "\n",
    "from __future__ import print_function\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad024b46",
   "metadata": {},
   "source": [
    "# Spark config due to broadcast size\n",
    "\n",
    "# SparkConf conf = new SparkConf();\n",
    "# conf.setMaster(\"local[*]\");\n",
    "# conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "import org.apache.spark.{SparkContext, SparkConf}\n",
    "\n",
    "val conf = new SparkConf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "val sc = new SparkContext(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f31a4b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/08 21:16:50 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "22/04/08 21:16:50 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "22/04/08 21:16:50 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/04/08 21:16:50 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    }
   ],
   "source": [
    "# For the model features, we may need to revisit once we decide on what features to include and predict on.\n",
    "# For now, my idea is to predict whether or not a case is closed\n",
    "\n",
    "# 'statusI', 'categoryI','complaint_typeI','descriptorI','sourceI','police_districtI'\n",
    "\n",
    "def vector_from_inputs(r):\n",
    "    return(r[\"statusI\"], Vectors.dense(float(r[\"categoryI\"]),\n",
    "                                           float(r[\"complaint_typeI\"]),\n",
    "                                           float(r[\"descriptorI\"]),\n",
    "                                           float(r[\"sourceI\"]),\n",
    "                                           #float(r[\"latitude\"]),\n",
    "                                           #float(r[\"longitude\"],\n",
    "                                           float(r[\"neighborhoodI\"]),\n",
    "                                           float(r[\"police_districtI\"])))\n",
    "\n",
    "spark = SparkSession.builder.config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar').config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaa56b3",
   "metadata": {},
   "source": [
    "## Get BigQuery Data\n",
    "\n",
    "For this model, we will use the below features to run a classification on case status. Now, there are likely other features that are more important or with more meaning (such as how long case is open) but for now we will start here to get our feet wet. \n",
    "\n",
    "Predicted Variable:\n",
    "\n",
    "    - status\n",
    "    \n",
    "Predictors:\n",
    "    \n",
    "    - category\n",
    "    - complaint_type\n",
    "    - descriptor\n",
    "    - source\n",
    "    - latitude\n",
    "    - longitude\n",
    "    - police_district\n",
    "    \n",
    "**note:** I have specifically used the above listed columns 'lat'/'long' for location data vs. categorizing the listed neighborhoods. This could be done but we already have so many features we will have to index/encode that I don't want to overwhelm the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc186435",
   "metadata": {},
   "outputs": [],
   "source": [
    "crime = spark.read.format(\"bigquery\").option(\n",
    "    \"table\", \"lcmhng_biengine_tutorial.311_service_requests\").load()\n",
    "# Create a view so that Spark SQL queries can be run against the data.\n",
    "crime.createOrReplaceTempView(\"311_crime\")\n",
    "\n",
    "## -------------------ENSURE CLEAN DATA NO NULLS----------------\n",
    "\n",
    "# Original SELECT with Lat/Long moved to neighborhood due to data complexity\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT status, category, complaint_type, descriptor, source, neighborhood, police_district\n",
    "from 311_crime\n",
    "WHERE status is not null\n",
    "AND category is not null\n",
    "AND complaint_type is not null\n",
    "AND descriptor is not null\n",
    "AND source is not null\n",
    "AND neighborhood is not null\n",
    "AND police_district is not null\n",
    "\"\"\"\n",
    "crime_clean = spark.sql(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a5da0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+-----------------+----------------+---------------+\n",
      "|status|            category|      complaint_type|          descriptor|           source|    neighborhood|police_district|\n",
      "+------+--------------------+--------------------+--------------------+-----------------+----------------+---------------+\n",
      "|Closed|Rec and Park Requ...|Park - Neighborho...|          Irrigation|            Phone|Presidio Terrace|       RICHMOND|\n",
      "|Closed|    Damaged Property|Damaged Transit_S...|Transit_Shelter_P...|            Phone|   Outer Mission|      INGLESIDE|\n",
      "|Closed|       Child Request|Shared_SpacesObst...|Required_gap_for_...|Integrated Agency|Presidio Heights|       RICHMOND|\n",
      "|Closed|   Abandoned Vehicle|Abandoned Vehicle...|DPT Abandoned Veh...|            Phone|   Miraloma Park|      INGLESIDE|\n",
      "|Closed|    Tree Maintenance|Trees - Overgrown...|      Blocking_signs|            Phone|      Parkmerced|        TARAVAL|\n",
      "|Closed|    Tree Maintenance| Trees - Landscaping|               Other|   Mobile/Open311|     West Portal|        TARAVAL|\n",
      "|Closed|   Abandoned Vehicle|Abandoned Vehicle...|green - Toyota - ...|              Web| Sherwood Forest|      INGLESIDE|\n",
      "|Closed|Blocked Street or...|Blocked_Parking_S...|Construction_equi...|   Mobile/Open311|       Mint Hill|           PARK|\n",
      "|Closed|         Sign Repair| Sign - Painted_Over|       DPT Sign Shop|            Phone|      Cow Hollow|       NORTHERN|\n",
      "|Closed|    Tree Maintenance|Trees - Damaging_...|Lifted_sidewalk_t...|            Phone|   Outer Mission|      INGLESIDE|\n",
      "+------+--------------------+--------------------+--------------------+-----------------+----------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "crime_clean.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6769ed33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=============================>                             (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4904881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(crime_clean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8ae551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Seeing as above the number of rows is huge we are just going to take a random sample of 1% of the data as that appears to be\n",
    "## the rough threshold to where this will run without a reconfigure\n",
    "\n",
    "crime_clean = crime_clean.sample(fraction=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70943f1",
   "metadata": {},
   "source": [
    "## Create indexed columns for model similar to previous exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c6b9cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#First index columns\n",
    "data_index = StringIndexer(inputCols=['status', 'category','complaint_type','descriptor','source','police_district', 'neighborhood'], \n",
    "                           outputCols=['statusI', 'categoryI','complaint_typeI','descriptorI','sourceI','police_districtI', 'neighborhoodI'])\n",
    "\n",
    "data_indexed = data_index.fit(crime_clean).transform(crime_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1512205c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+-----------------+--------------------+---------------+----------------+-----------+-------+---------+---------------+-------------+-------+\n",
      "|status|            category|      complaint_type|          descriptor|           source|        neighborhood|police_district|police_districtI|descriptorI|statusI|categoryI|complaint_typeI|neighborhoodI|sourceI|\n",
      "+------+--------------------+--------------------+--------------------+-----------------+--------------------+---------------+----------------+-----------+-------+---------+---------------+-------------+-------+\n",
      "|Closed|       SFHA Requests|          Electrical|Electrical - Routine|            Phone|      Merced Heights|        TARAVAL|             5.0|      137.0|    0.0|     14.0|           60.0|         58.0|    1.0|\n",
      "|Closed|       SFHA Requests|            Plumbing|  Plumbing - Routine|            Phone|           Sunnydale|      INGLESIDE|             2.0|      103.0|    0.0|     14.0|           33.0|         65.0|    1.0|\n",
      "|Closed|Street and Sidewa...|         Bulky Items|            Mattress|   Mobile/Open311|       Outer Mission|        TARAVAL|             5.0|        5.0|    0.0|      0.0|            0.0|         59.0|    0.0|\n",
      "|Closed|Street and Sidewa...|    General Cleaning| Other Loose Garbage|            Phone|  Central Waterfront|        BAYVIEW|             6.0|        0.0|    0.0|      0.0|            1.0|         54.0|    1.0|\n",
      "|Closed|Street and Sidewa...|         Bulky Items|        Refrigerator|Integrated Agency|Candlestick Point...|        BAYVIEW|             6.0|        7.0|    0.0|      0.0|            0.0|        110.0|    3.0|\n",
      "|Closed|Street and Sidewa...| Hazardous Materials|               Glass|            Phone|   Clarendon Heights|           PARK|             8.0|       22.0|    0.0|      0.0|           12.0|         96.0|    1.0|\n",
      "|Closed|Street and Sidewa...|    General Cleaning| Other Loose Garbage|   Mobile/Open311|  Central Waterfront|        BAYVIEW|             6.0|        0.0|    0.0|      0.0|            1.0|         54.0|    0.0|\n",
      "|Closed|    Tree Maintenance|Trees - Damaged_Tree|         Fallen_tree|            Phone|      Corona Heights|           PARK|             8.0|       54.0|    0.0|      8.0|           30.0|         73.0|    1.0|\n",
      "|Closed|       MUNI Feedback|MUNI - Conduct_In...|201_Pass_Up_Did_N...|            Phone|  Ingleside Terraces|        TARAVAL|             5.0|       41.0|    0.0|     15.0|           43.0|         99.0|    1.0|\n",
      "|Closed| Parking Enforcement| Parking_on_Sidewalk|      WHITE - FORD -|            Phone|     Midtown Terrace|           PARK|             8.0|     3130.0|    0.0|      3.0|           15.0|         97.0|    1.0|\n",
      "+------+--------------------+--------------------+--------------------+-----------------+--------------------+---------------+----------------+-----------+-------+---------+---------------+-------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_indexed.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e123b4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will only need the indexed versions of the columns\n",
    "\n",
    "model_data = data_indexed.drop('status', 'category','complaint_type','descriptor','source','police_district', 'neighborhood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8094f3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+-------+---------+---------------+-------------+-------+\n",
      "|police_districtI|descriptorI|statusI|categoryI|complaint_typeI|neighborhoodI|sourceI|\n",
      "+----------------+-----------+-------+---------+---------------+-------------+-------+\n",
      "|             5.0|      137.0|    0.0|     14.0|           60.0|         58.0|    1.0|\n",
      "+----------------+-----------+-------+---------+---------------+-------------+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "489edf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, features: vector]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, move to training data\n",
    "\n",
    "training_data = model_data.rdd.map(vector_from_inputs).toDF([\"label\",\n",
    "                                                             \"features\"])\n",
    "training_data.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f8c06d",
   "metadata": {},
   "source": [
    "**checking size of data as I had issues below**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1b30575e",
   "metadata": {},
   "source": [
    "print((training_data.count(), len(training_data.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a27c270",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Now, in light of instructions we will choose a different classification model for this analysis for variability. I'll start with a Log and see where that goes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3071893d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/08 21:17:45 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/04/08 21:17:45 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "logr = LogisticRegression(maxIter=5, regParam=0.3, elasticNetParam=0.7)\n",
    "model = logr.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad8f675f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9868239272532322"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bfa3d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, features: vector]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bc0910b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:============================================>           (12 + 2) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0\n",
      "0  47858.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Additional Model Summary Notes\n",
    "# Took some notes to derive a confusion matrix from the output\n",
    "\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "predictionAndLabels = model.transform(training_data).select('label', 'prediction')\n",
    "\n",
    "#important: need to cast to float type, and order by prediction, else it won't work\n",
    "#preds_and_labels = predictions.select(['prediction','features']).withColumn('label', F.col('d').cast(FloatType())).orderBy('prediction')\n",
    "\n",
    "#select only prediction and label columns\n",
    "#preds_and_labels = preds_and_labels.select(['prediction','label'])\n",
    "\n",
    "#predictionAndLabels = SparkContext.parallelize(predictions, training_data.label)\n",
    "\n",
    "metrics = MulticlassMetrics(predictionAndLabels.rdd.map(lambda x: tuple(map(float, x))))\n",
    "\n",
    "#print(metrics.confusionMatrix().toArray())\n",
    "\n",
    "confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "labels = [int(l) for l in metrics.call('labels')]\n",
    "confusion_matrix = pd.DataFrame(confusion_matrix , index=labels, columns=labels)\n",
    "\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce0254d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

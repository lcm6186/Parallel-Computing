{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Linear Regression Using SparkML in GCP DataProc\n",
    "\n",
    "Run a linear regression using Apache Spark ML.\n",
    "\n",
    "In the following PySpark (Spark Python API) code, we take the following actions:\n",
    "\n",
    "  * Load a previously created linear regression (BigQuery) input table\n",
    "    into our Cloud Dataproc Spark cluster as an RDD (Resilient\n",
    "    Distributed Dataset)\n",
    "  * Transform the RDD into a Spark Dataframe\n",
    "  * Vectorize the features on which the model will be trained\n",
    "  * Compute a linear regression using Spark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "from __future__ import print_function\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql.session import SparkSession\n",
    "# The imports, above, allow us to access SparkML features specific to linear\n",
    "# regression as well as the Vectors types\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Next, we define a function that collects the features of interest\n",
    "(mother_age, father_age, and gestation_weeks) into a vector.\n",
    "\n",
    "\n",
    "Package the vector in a tuple containing the label (`weight_pounds`) for that\n",
    "row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/06 16:20:06 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "22/04/06 16:20:06 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "22/04/06 16:20:06 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/04/06 16:20:07 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    }
   ],
   "source": [
    "def vector_from_inputs(r):\n",
    "    return (r[\"weight_pounds\"], Vectors.dense(float(r[\"mother_age\"]),\n",
    "                                            float(r[\"father_age\"]),\n",
    "                                            float(r[\"gestation_weeks\"]),\n",
    "                                            float(r[\"weight_gain_pounds\"]),\n",
    "                                            float(r[\"apgar_5min\"])))\n",
    "\n",
    "# sc = SparkContext()\n",
    "# spark = SparkSession(sc)\n",
    "\n",
    "spark = SparkSession.builder.config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve data from BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "natality_data = spark.read.format(\"bigquery\").option(\n",
    "    \"table\", \"natality_regression_lcmhng.regression_input\").load()\n",
    "# Create a view so that Spark SQL queries can be run against the data.\n",
    "natality_data.createOrReplaceTempView(\"natality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure we do not have null values in the retrieved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a precaution, run a query in Spark SQL to ensure no NULL values exist.\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "from natality\n",
    "where weight_pounds is not null\n",
    "and mother_age is not null\n",
    "and father_age is not null\n",
    "and gestation_weeks is not null\n",
    "\"\"\"\n",
    "clean_data = spark.sql(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model and get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, features: vector]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an input DataFrame for Spark ML using the above function.\n",
    "training_data = clean_data.rdd.map(vector_from_inputs).toDF([\"label\",\n",
    "                                                             \"features\"])\n",
    "training_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/06 16:31:33 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/04/06 16:31:33 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "22/04/06 16:31:33 WARN com.github.fommil.netlib.LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "22/04/06 16:31:33 WARN com.github.fommil.netlib.LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:[0.0166657454631094,-0.0029675198396670547,0.23571439297525,0.002130020702160164,-0.0004857725171350398]\n",
      "Intercept:-2.2613033091401618\n",
      "R^2:0.2952005790467246\n",
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "|  1.5200177066924123|\n",
      "| 0.43406716816449453|\n",
      "|  1.1069502679100047|\n",
      "|   -2.39374606430762|\n",
      "| -0.3422831562811899|\n",
      "| -1.7954576684049028|\n",
      "|  0.4321400465038314|\n",
      "| -2.1437081115992154|\n",
      "| -0.5338981682655755|\n",
      "|-0.47571385742194483|\n",
      "|  0.6014382599197452|\n",
      "|   0.610696485413726|\n",
      "|  0.8446255763004125|\n",
      "|   -2.51233134747541|\n",
      "| 0.03278077088410214|\n",
      "| 0.15503577486861353|\n",
      "| -0.3360047199767555|\n",
      "|   2.098502228284863|\n",
      "|-0.25160131787674267|\n",
      "|  1.1089628643295146|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Construct a new LinearRegression object and fit the training data.\n",
    "lr = LinearRegression(maxIter=5, regParam=0.2, solver=\"normal\")\n",
    "model = lr.fit(training_data)\n",
    "# Print the model summary.\n",
    "print(\"Coefficients:\" + str(model.coefficients))\n",
    "print(\"Intercept:\" + str(model.intercept))\n",
    "print(\"R^2:\" + str(model.summary.r2))\n",
    "model.summary.residuals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
